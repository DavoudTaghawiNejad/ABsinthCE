Over the long weekend I run a technical experiment to make ABCE working faster on high performance computers. I write this document to update you, but mainly to write down some technical considerations about ABCE, that need to be documented for future developments of ABCE.

Messaging is at heard of any simulation platform that wants to achieve replaceable and calibrateable results, while being multithreaded. This is because only with messaging guarantees consistent interactions over multiple runs. Further messaging guarantees a consistent representation of time.

ABCE has over the time and over experiments have had three messaging backends: Initially it was using pyZMQ forwarding message over a custom written message broker and time giver. (see attached PDF for a picture of the architecture). This implementation was in pure python using the pyZMQ library( which in-turn uses C++ binding. The use of C++ bindings however was handled by pyZMQ and did not complicate ABCEs installation process.) This architecture was scalable and ready to work on High Performance Computers. The use of zeromq without the need for INPROC, did even allow it to put agents an different computers or distribute the over the internet. It had however a fatal and a non fatal flaw. It did not run on windows as the multiprocessing module of python uses a fork and join model in windows that does not work with zeromq. It was also hugely complicated. (again, just look at the PDF). 

With the current production and development version we decided to use pure python (at least for messaging) and messaging as provided by python. This is slower, but safer as it runs on virtually any system. It works simple; in the development version we create a class ProcessorGroup for each processor core we have. (one can create more or less processor-groups). When we create agent groups each agent is distributed according to its number to a processor group. For example if we have 4 processor-groups agent 1, 5, 9, 13... are in processorgroup 1. (id % num_processorgroups). This ensures that we know where each agent is without having to look it up. Note that a processor-group can have agents of different groups, for example households and firms. 
When we run the simulation the processor-groups are executed in parallel. The processor-groups execute all agents that belong to them sequentially. Which leads to a full use of the parallel computing ability. Agents communicate with other agents only between rounds. The order of messages arriving is randomized, but the randomization depends on a random seed so its reproduceable. While agents are executed they produce messages to other agent, these are returned to the process-group, presorted by destination processor-group. The process-group keeps the messages for its own agents and returns the messages for other agents to the main process. From the main process these messages are given to the according process-groups, by the pool command that executes the agents in parallel for the next sub round. (as process-groups keep messages for their own process-group, when we execute the simulation with a single process the message passing is circumvented and we have non of the multiprocessing overhead). The number of agents is only bound by the memory( and __slots__ can decrease the memory foot print). 

Now, this is all pretty but its not the fastest possible way. The problem is that we copy the messages twice passing it out of the processor-group and back in; in python. 
Enters the Absinth experiment, which I did over the weekend: The idea is to do the whole message passing in C using cython and zeromq (in C not pyZMQ). The experiment was done as a proof of concept rather than on the actual ABCE code. We created 'virtual' CAgents each virtual CAgent ownes a python Agent, which the modeller provides. The python agents use abce.Messaging and abce.Trade to pass (C)messages to the CAgent via return, which than get send in C via PMQ on an INPROC socket to other CAgents. This all is done without the GIL in C using cython which means that we have shared memory concurrency. 
In principle it works, however for speed we use INPROC sockets. In theory every context could have 65530 sockets, which forces us to use several contexts, with their own address daisy-chained via C. This would allow us the whopping number of 1.073.545.225, many more agents than fit into the memory of even the most sophisticated supercomputer. Here comes the drag. On Mac we are limited to 2042 sockets. Half of them we need for inter context communication, so we are stuck with 1021 ** 2 = 1.042.441 agents. Possibly we could create a master context and have 2041 **2 = 4165681, this would hovever be slower, possibly only by a few nano seconds. Theoretically one can increase the socket number of a mac to 65530, but that involves booting the computer to recovery and entering some obscure code into the terminal. 

What I have learned from the two implementation is that the time giver concept is not ideal in the original pyZMQ implementation all agents would be always running and waiting for messages and a 'go' from the time giver. In the pure python version the timing is simply done by calling the processor groups. If we pick up project Absinth in order to speed up ABCE and make it high performance computing friendly a hibrid would be best messages should be passed via message passing, but the execution should be done via pythons build in means. For example pool. That simplifies the code tremendously. 

In the fare future project Absinth should definitely be taken up. The code could be designed in such a way that it comes in a separate package, so that abce's installation remains simple.Then one could replace 'import abce.Simulation' with 'import abce_hpc.Simulation' and run ABCE in high performance mode on a linux server or super computer.

In conclusion we have close to a proof that ABCE could be adapted to high performance computing in C, so ABCE will not fall on its face as predicted by some. In practice however this modifications would make the code much more complicated which with the current man power is not maintainable. The current development branch is pretty elegant and message passing takes less than 100 lines in python. 
